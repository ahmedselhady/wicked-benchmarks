{
  "results": {
    "mmlu_redux": {
      "alias": "mmlu_redux"
    },
    "mmlu_redux_anatomy": {
      "alias": " - anatomy",
      "acc,none": 0.64,
      "acc_stderr,none": 0.04824181513244218
    },
    "mmlu_redux_astronomy": {
      "alias": " - astronomy",
      "acc,none": 0.62,
      "acc_stderr,none": 0.04878317312145633
    },
    "mmlu_redux_business_ethics": {
      "alias": " - business_ethics",
      "acc,none": 0.65,
      "acc_stderr,none": 0.0479372485441102
    },
    "mmlu_redux_clinical_knowledge": {
      "alias": " - clinical_knowledge",
      "acc,none": 0.67,
      "acc_stderr,none": 0.04725815626252606
    },
    "mmlu_redux_college_chemistry": {
      "alias": " - college_chemistry",
      "acc,none": 0.41,
      "acc_stderr,none": 0.04943110704237102
    },
    "mmlu_redux_college_computer_science": {
      "alias": " - college_computer_science",
      "acc,none": 0.47,
      "acc_stderr,none": 0.05016135580465919
    },
    "mmlu_redux_college_mathematics": {
      "alias": " - college_mathematics",
      "acc,none": 0.37,
      "acc_stderr,none": 0.04852365870939099
    },
    "mmlu_redux_college_medicine": {
      "alias": " - college_medicine",
      "acc,none": 0.63,
      "acc_stderr,none": 0.04852365870939099
    },
    "mmlu_redux_college_physics": {
      "alias": " - college_physics",
      "acc,none": 0.43,
      "acc_stderr,none": 0.04975698519562428
    },
    "mmlu_redux_conceptual_physics": {
      "alias": " - conceptual_physics",
      "acc,none": 0.5,
      "acc_stderr,none": 0.050251890762960605
    },
    "mmlu_redux_econometrics": {
      "alias": " - econometrics",
      "acc,none": 0.42,
      "acc_stderr,none": 0.049604496374885836
    },
    "mmlu_redux_electrical_engineering": {
      "alias": " - electrical_engineering",
      "acc,none": 0.58,
      "acc_stderr,none": 0.049604496374885836
    },
    "mmlu_redux_formal_logic": {
      "alias": " - formal_logic",
      "acc,none": 0.35,
      "acc_stderr,none": 0.0479372485441102
    },
    "mmlu_redux_global_facts": {
      "alias": " - global_facts",
      "acc,none": 0.34,
      "acc_stderr,none": 0.04760952285695236
    },
    "mmlu_redux_high_school_chemistry": {
      "alias": " - high_school_chemistry",
      "acc,none": 0.46,
      "acc_stderr,none": 0.05009082659620332
    },
    "mmlu_redux_high_school_geography": {
      "alias": " - high_school_geography",
      "acc,none": 0.72,
      "acc_stderr,none": 0.04512608598542127
    },
    "mmlu_redux_high_school_macroeconomics": {
      "alias": " - high_school_macroeconomics",
      "acc,none": 0.52,
      "acc_stderr,none": 0.050211673156867795
    },
    "mmlu_redux_high_school_mathematics": {
      "alias": " - high_school_mathematics",
      "acc,none": 0.36,
      "acc_stderr,none": 0.048241815132442176
    },
    "mmlu_redux_high_school_physics": {
      "alias": " - high_school_physics",
      "acc,none": 0.34,
      "acc_stderr,none": 0.04760952285695235
    },
    "mmlu_redux_high_school_statistics": {
      "alias": " - high_school_statistics",
      "acc,none": 0.51,
      "acc_stderr,none": 0.05024183937956912
    },
    "mmlu_redux_high_school_us_history": {
      "alias": " - high_school_us_history",
      "acc,none": 0.82,
      "acc_stderr,none": 0.038612291966536955
    },
    "mmlu_redux_human_aging": {
      "alias": " - human_aging",
      "acc,none": 0.66,
      "acc_stderr,none": 0.04760952285695237
    },
    "mmlu_redux_logical_fallacies": {
      "alias": " - logical_fallacies",
      "acc,none": 0.82,
      "acc_stderr,none": 0.03861229196653693
    },
    "mmlu_redux_machine_learning": {
      "alias": " - machine_learning",
      "acc,none": 0.54,
      "acc_stderr,none": 0.05009082659620333
    },
    "mmlu_redux_miscellaneous": {
      "alias": " - miscellaneous",
      "acc,none": 0.81,
      "acc_stderr,none": 0.03942772444036623
    },
    "mmlu_redux_philosophy": {
      "alias": " - philosophy",
      "acc,none": 0.66,
      "acc_stderr,none": 0.04760952285695238
    },
    "mmlu_redux_professional_accounting": {
      "alias": " - professional_accounting",
      "acc,none": 0.49,
      "acc_stderr,none": 0.05024183937956912
    },
    "mmlu_redux_professional_law": {
      "alias": " - professional_law",
      "acc,none": 0.42,
      "acc_stderr,none": 0.049604496374885836
    },
    "mmlu_redux_public_relations": {
      "alias": " - public_relations",
      "acc,none": 0.75,
      "acc_stderr,none": 0.04351941398892446
    },
    "mmlu_redux_virology": {
      "alias": " - virology",
      "acc,none": 0.53,
      "acc_stderr,none": 0.05016135580465918
    }
  },
  "groups": {
    "mmlu_redux": {
      "alias": "mmlu_redux"
    }
  },
  "group_subtasks": {
    "mmlu_redux": [
      "mmlu_redux_machine_learning",
      "mmlu_redux_professional_law",
      "mmlu_redux_logical_fallacies",
      "mmlu_redux_high_school_macroeconomics",
      "mmlu_redux_conceptual_physics",
      "mmlu_redux_human_aging",
      "mmlu_redux_philosophy",
      "mmlu_redux_virology",
      "mmlu_redux_clinical_knowledge",
      "mmlu_redux_college_chemistry",
      "mmlu_redux_astronomy",
      "mmlu_redux_public_relations",
      "mmlu_redux_miscellaneous",
      "mmlu_redux_college_medicine",
      "mmlu_redux_high_school_mathematics",
      "mmlu_redux_high_school_physics",
      "mmlu_redux_anatomy",
      "mmlu_redux_global_facts",
      "mmlu_redux_high_school_statistics",
      "mmlu_redux_high_school_us_history",
      "mmlu_redux_high_school_chemistry",
      "mmlu_redux_college_computer_science",
      "mmlu_redux_econometrics",
      "mmlu_redux_high_school_geography",
      "mmlu_redux_professional_accounting",
      "mmlu_redux_business_ethics",
      "mmlu_redux_formal_logic",
      "mmlu_redux_college_mathematics",
      "mmlu_redux_electrical_engineering",
      "mmlu_redux_college_physics"
    ]
  },
  "configs": {
    "mmlu_redux_anatomy": {
      "task": "mmlu_redux_anatomy",
      "task_alias": "anatomy",
      "dataset_path": "edinburgh-dawg/mmlu-redux",
      "dataset_name": "anatomy",
      "dataset_kwargs": {
        "trust_remote_code": true
      },
      "test_split": "test",
      "fewshot_split": "test",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0
      }
    },
    "mmlu_redux_astronomy": {
      "task": "mmlu_redux_astronomy",
      "task_alias": "astronomy",
      "dataset_path": "edinburgh-dawg/mmlu-redux",
      "dataset_name": "astronomy",
      "dataset_kwargs": {
        "trust_remote_code": true
      },
      "test_split": "test",
      "fewshot_split": "test",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0
      }
    },
    "mmlu_redux_business_ethics": {
      "task": "mmlu_redux_business_ethics",
      "task_alias": "business_ethics",
      "dataset_path": "edinburgh-dawg/mmlu-redux",
      "dataset_name": "business_ethics",
      "dataset_kwargs": {
        "trust_remote_code": true
      },
      "test_split": "test",
      "fewshot_split": "test",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0
      }
    },
    "mmlu_redux_clinical_knowledge": {
      "task": "mmlu_redux_clinical_knowledge",
      "task_alias": "clinical_knowledge",
      "dataset_path": "edinburgh-dawg/mmlu-redux",
      "dataset_name": "clinical_knowledge",
      "dataset_kwargs": {
        "trust_remote_code": true
      },
      "test_split": "test",
      "fewshot_split": "test",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0
      }
    },
    "mmlu_redux_college_chemistry": {
      "task": "mmlu_redux_college_chemistry",
      "task_alias": "college_chemistry",
      "dataset_path": "edinburgh-dawg/mmlu-redux",
      "dataset_name": "college_chemistry",
      "dataset_kwargs": {
        "trust_remote_code": true
      },
      "test_split": "test",
      "fewshot_split": "test",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0
      }
    },
    "mmlu_redux_college_computer_science": {
      "task": "mmlu_redux_college_computer_science",
      "task_alias": "college_computer_science",
      "dataset_path": "edinburgh-dawg/mmlu-redux",
      "dataset_name": "college_computer_science",
      "dataset_kwargs": {
        "trust_remote_code": true
      },
      "test_split": "test",
      "fewshot_split": "test",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0
      }
    },
    "mmlu_redux_college_mathematics": {
      "task": "mmlu_redux_college_mathematics",
      "task_alias": "college_mathematics",
      "dataset_path": "edinburgh-dawg/mmlu-redux",
      "dataset_name": "college_mathematics",
      "dataset_kwargs": {
        "trust_remote_code": true
      },
      "test_split": "test",
      "fewshot_split": "test",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0
      }
    },
    "mmlu_redux_college_medicine": {
      "task": "mmlu_redux_college_medicine",
      "task_alias": "college_medicine",
      "dataset_path": "edinburgh-dawg/mmlu-redux",
      "dataset_name": "college_medicine",
      "dataset_kwargs": {
        "trust_remote_code": true
      },
      "test_split": "test",
      "fewshot_split": "test",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0
      }
    },
    "mmlu_redux_college_physics": {
      "task": "mmlu_redux_college_physics",
      "task_alias": "college_physics",
      "dataset_path": "edinburgh-dawg/mmlu-redux",
      "dataset_name": "college_physics",
      "dataset_kwargs": {
        "trust_remote_code": true
      },
      "test_split": "test",
      "fewshot_split": "test",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0
      }
    },
    "mmlu_redux_conceptual_physics": {
      "task": "mmlu_redux_conceptual_physics",
      "task_alias": "conceptual_physics",
      "dataset_path": "edinburgh-dawg/mmlu-redux",
      "dataset_name": "conceptual_physics",
      "dataset_kwargs": {
        "trust_remote_code": true
      },
      "test_split": "test",
      "fewshot_split": "test",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0
      }
    },
    "mmlu_redux_econometrics": {
      "task": "mmlu_redux_econometrics",
      "task_alias": "econometrics",
      "dataset_path": "edinburgh-dawg/mmlu-redux",
      "dataset_name": "econometrics",
      "dataset_kwargs": {
        "trust_remote_code": true
      },
      "test_split": "test",
      "fewshot_split": "test",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0
      }
    },
    "mmlu_redux_electrical_engineering": {
      "task": "mmlu_redux_electrical_engineering",
      "task_alias": "electrical_engineering",
      "dataset_path": "edinburgh-dawg/mmlu-redux",
      "dataset_name": "electrical_engineering",
      "dataset_kwargs": {
        "trust_remote_code": true
      },
      "test_split": "test",
      "fewshot_split": "test",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0
      }
    },
    "mmlu_redux_formal_logic": {
      "task": "mmlu_redux_formal_logic",
      "task_alias": "formal_logic",
      "dataset_path": "edinburgh-dawg/mmlu-redux",
      "dataset_name": "formal_logic",
      "dataset_kwargs": {
        "trust_remote_code": true
      },
      "test_split": "test",
      "fewshot_split": "test",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0
      }
    },
    "mmlu_redux_global_facts": {
      "task": "mmlu_redux_global_facts",
      "task_alias": "global_facts",
      "dataset_path": "edinburgh-dawg/mmlu-redux",
      "dataset_name": "global_facts",
      "dataset_kwargs": {
        "trust_remote_code": true
      },
      "test_split": "test",
      "fewshot_split": "test",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0
      }
    },
    "mmlu_redux_high_school_chemistry": {
      "task": "mmlu_redux_high_school_chemistry",
      "task_alias": "high_school_chemistry",
      "dataset_path": "edinburgh-dawg/mmlu-redux",
      "dataset_name": "high_school_chemistry",
      "dataset_kwargs": {
        "trust_remote_code": true
      },
      "test_split": "test",
      "fewshot_split": "test",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0
      }
    },
    "mmlu_redux_high_school_geography": {
      "task": "mmlu_redux_high_school_geography",
      "task_alias": "high_school_geography",
      "dataset_path": "edinburgh-dawg/mmlu-redux",
      "dataset_name": "high_school_geography",
      "dataset_kwargs": {
        "trust_remote_code": true
      },
      "test_split": "test",
      "fewshot_split": "test",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0
      }
    },
    "mmlu_redux_high_school_macroeconomics": {
      "task": "mmlu_redux_high_school_macroeconomics",
      "task_alias": "high_school_macroeconomics",
      "dataset_path": "edinburgh-dawg/mmlu-redux",
      "dataset_name": "high_school_macroeconomics",
      "dataset_kwargs": {
        "trust_remote_code": true
      },
      "test_split": "test",
      "fewshot_split": "test",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0
      }
    },
    "mmlu_redux_high_school_mathematics": {
      "task": "mmlu_redux_high_school_mathematics",
      "task_alias": "high_school_mathematics",
      "dataset_path": "edinburgh-dawg/mmlu-redux",
      "dataset_name": "high_school_mathematics",
      "dataset_kwargs": {
        "trust_remote_code": true
      },
      "test_split": "test",
      "fewshot_split": "test",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0
      }
    },
    "mmlu_redux_high_school_physics": {
      "task": "mmlu_redux_high_school_physics",
      "task_alias": "high_school_physics",
      "dataset_path": "edinburgh-dawg/mmlu-redux",
      "dataset_name": "high_school_physics",
      "dataset_kwargs": {
        "trust_remote_code": true
      },
      "test_split": "test",
      "fewshot_split": "test",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0
      }
    },
    "mmlu_redux_high_school_statistics": {
      "task": "mmlu_redux_high_school_statistics",
      "task_alias": "high_school_statistics",
      "dataset_path": "edinburgh-dawg/mmlu-redux",
      "dataset_name": "high_school_statistics",
      "dataset_kwargs": {
        "trust_remote_code": true
      },
      "test_split": "test",
      "fewshot_split": "test",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0
      }
    },
    "mmlu_redux_high_school_us_history": {
      "task": "mmlu_redux_high_school_us_history",
      "task_alias": "high_school_us_history",
      "dataset_path": "edinburgh-dawg/mmlu-redux",
      "dataset_name": "high_school_us_history",
      "dataset_kwargs": {
        "trust_remote_code": true
      },
      "test_split": "test",
      "fewshot_split": "test",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0
      }
    },
    "mmlu_redux_human_aging": {
      "task": "mmlu_redux_human_aging",
      "task_alias": "human_aging",
      "dataset_path": "edinburgh-dawg/mmlu-redux",
      "dataset_name": "human_aging",
      "dataset_kwargs": {
        "trust_remote_code": true
      },
      "test_split": "test",
      "fewshot_split": "test",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0
      }
    },
    "mmlu_redux_logical_fallacies": {
      "task": "mmlu_redux_logical_fallacies",
      "task_alias": "logical_fallacies",
      "dataset_path": "edinburgh-dawg/mmlu-redux",
      "dataset_name": "logical_fallacies",
      "dataset_kwargs": {
        "trust_remote_code": true
      },
      "test_split": "test",
      "fewshot_split": "test",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0
      }
    },
    "mmlu_redux_machine_learning": {
      "task": "mmlu_redux_machine_learning",
      "task_alias": "machine_learning",
      "dataset_path": "edinburgh-dawg/mmlu-redux",
      "dataset_name": "machine_learning",
      "dataset_kwargs": {
        "trust_remote_code": true
      },
      "test_split": "test",
      "fewshot_split": "test",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0
      }
    },
    "mmlu_redux_miscellaneous": {
      "task": "mmlu_redux_miscellaneous",
      "task_alias": "miscellaneous",
      "dataset_path": "edinburgh-dawg/mmlu-redux",
      "dataset_name": "miscellaneous",
      "dataset_kwargs": {
        "trust_remote_code": true
      },
      "test_split": "test",
      "fewshot_split": "test",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0
      }
    },
    "mmlu_redux_philosophy": {
      "task": "mmlu_redux_philosophy",
      "task_alias": "philosophy",
      "dataset_path": "edinburgh-dawg/mmlu-redux",
      "dataset_name": "philosophy",
      "dataset_kwargs": {
        "trust_remote_code": true
      },
      "test_split": "test",
      "fewshot_split": "test",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0
      }
    },
    "mmlu_redux_professional_accounting": {
      "task": "mmlu_redux_professional_accounting",
      "task_alias": "professional_accounting",
      "dataset_path": "edinburgh-dawg/mmlu-redux",
      "dataset_name": "professional_accounting",
      "dataset_kwargs": {
        "trust_remote_code": true
      },
      "test_split": "test",
      "fewshot_split": "test",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0
      }
    },
    "mmlu_redux_professional_law": {
      "task": "mmlu_redux_professional_law",
      "task_alias": "professional_law",
      "dataset_path": "edinburgh-dawg/mmlu-redux",
      "dataset_name": "professional_law",
      "dataset_kwargs": {
        "trust_remote_code": true
      },
      "test_split": "test",
      "fewshot_split": "test",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0
      }
    },
    "mmlu_redux_public_relations": {
      "task": "mmlu_redux_public_relations",
      "task_alias": "public_relations",
      "dataset_path": "edinburgh-dawg/mmlu-redux",
      "dataset_name": "public_relations",
      "dataset_kwargs": {
        "trust_remote_code": true
      },
      "test_split": "test",
      "fewshot_split": "test",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0
      }
    },
    "mmlu_redux_virology": {
      "task": "mmlu_redux_virology",
      "task_alias": "virology",
      "dataset_path": "edinburgh-dawg/mmlu-redux",
      "dataset_name": "virology",
      "dataset_kwargs": {
        "trust_remote_code": true
      },
      "test_split": "test",
      "fewshot_split": "test",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0
      }
    }
  },
  "versions": {
    "mmlu_redux": 2.0,
    "mmlu_redux_anatomy": 1.0,
    "mmlu_redux_astronomy": 1.0,
    "mmlu_redux_business_ethics": 1.0,
    "mmlu_redux_clinical_knowledge": 1.0,
    "mmlu_redux_college_chemistry": 1.0,
    "mmlu_redux_college_computer_science": 1.0,
    "mmlu_redux_college_mathematics": 1.0,
    "mmlu_redux_college_medicine": 1.0,
    "mmlu_redux_college_physics": 1.0,
    "mmlu_redux_conceptual_physics": 1.0,
    "mmlu_redux_econometrics": 1.0,
    "mmlu_redux_electrical_engineering": 1.0,
    "mmlu_redux_formal_logic": 1.0,
    "mmlu_redux_global_facts": 1.0,
    "mmlu_redux_high_school_chemistry": 1.0,
    "mmlu_redux_high_school_geography": 1.0,
    "mmlu_redux_high_school_macroeconomics": 1.0,
    "mmlu_redux_high_school_mathematics": 1.0,
    "mmlu_redux_high_school_physics": 1.0,
    "mmlu_redux_high_school_statistics": 1.0,
    "mmlu_redux_high_school_us_history": 1.0,
    "mmlu_redux_human_aging": 1.0,
    "mmlu_redux_logical_fallacies": 1.0,
    "mmlu_redux_machine_learning": 1.0,
    "mmlu_redux_miscellaneous": 1.0,
    "mmlu_redux_philosophy": 1.0,
    "mmlu_redux_professional_accounting": 1.0,
    "mmlu_redux_professional_law": 1.0,
    "mmlu_redux_public_relations": 1.0,
    "mmlu_redux_virology": 1.0
  },
  "n-shot": {
    "mmlu_redux_anatomy": 3,
    "mmlu_redux_astronomy": 3,
    "mmlu_redux_business_ethics": 3,
    "mmlu_redux_clinical_knowledge": 3,
    "mmlu_redux_college_chemistry": 3,
    "mmlu_redux_college_computer_science": 3,
    "mmlu_redux_college_mathematics": 3,
    "mmlu_redux_college_medicine": 3,
    "mmlu_redux_college_physics": 3,
    "mmlu_redux_conceptual_physics": 3,
    "mmlu_redux_econometrics": 3,
    "mmlu_redux_electrical_engineering": 3,
    "mmlu_redux_formal_logic": 3,
    "mmlu_redux_global_facts": 3,
    "mmlu_redux_high_school_chemistry": 3,
    "mmlu_redux_high_school_geography": 3,
    "mmlu_redux_high_school_macroeconomics": 3,
    "mmlu_redux_high_school_mathematics": 3,
    "mmlu_redux_high_school_physics": 3,
    "mmlu_redux_high_school_statistics": 3,
    "mmlu_redux_high_school_us_history": 3,
    "mmlu_redux_human_aging": 3,
    "mmlu_redux_logical_fallacies": 3,
    "mmlu_redux_machine_learning": 3,
    "mmlu_redux_miscellaneous": 3,
    "mmlu_redux_philosophy": 3,
    "mmlu_redux_professional_accounting": 3,
    "mmlu_redux_professional_law": 3,
    "mmlu_redux_public_relations": 3,
    "mmlu_redux_virology": 3
  },
  "higher_is_better": {
    "mmlu_redux": {
      "acc": true
    },
    "mmlu_redux_anatomy": {
      "acc": true
    },
    "mmlu_redux_astronomy": {
      "acc": true
    },
    "mmlu_redux_business_ethics": {
      "acc": true
    },
    "mmlu_redux_clinical_knowledge": {
      "acc": true
    },
    "mmlu_redux_college_chemistry": {
      "acc": true
    },
    "mmlu_redux_college_computer_science": {
      "acc": true
    },
    "mmlu_redux_college_mathematics": {
      "acc": true
    },
    "mmlu_redux_college_medicine": {
      "acc": true
    },
    "mmlu_redux_college_physics": {
      "acc": true
    },
    "mmlu_redux_conceptual_physics": {
      "acc": true
    },
    "mmlu_redux_econometrics": {
      "acc": true
    },
    "mmlu_redux_electrical_engineering": {
      "acc": true
    },
    "mmlu_redux_formal_logic": {
      "acc": true
    },
    "mmlu_redux_global_facts": {
      "acc": true
    },
    "mmlu_redux_high_school_chemistry": {
      "acc": true
    },
    "mmlu_redux_high_school_geography": {
      "acc": true
    },
    "mmlu_redux_high_school_macroeconomics": {
      "acc": true
    },
    "mmlu_redux_high_school_mathematics": {
      "acc": true
    },
    "mmlu_redux_high_school_physics": {
      "acc": true
    },
    "mmlu_redux_high_school_statistics": {
      "acc": true
    },
    "mmlu_redux_high_school_us_history": {
      "acc": true
    },
    "mmlu_redux_human_aging": {
      "acc": true
    },
    "mmlu_redux_logical_fallacies": {
      "acc": true
    },
    "mmlu_redux_machine_learning": {
      "acc": true
    },
    "mmlu_redux_miscellaneous": {
      "acc": true
    },
    "mmlu_redux_philosophy": {
      "acc": true
    },
    "mmlu_redux_professional_accounting": {
      "acc": true
    },
    "mmlu_redux_professional_law": {
      "acc": true
    },
    "mmlu_redux_public_relations": {
      "acc": true
    },
    "mmlu_redux_virology": {
      "acc": true
    }
  },
  "n-samples": {
    "mmlu_redux_machine_learning": {
      "original": 100,
      "effective": 100
    },
    "mmlu_redux_professional_law": {
      "original": 100,
      "effective": 100
    },
    "mmlu_redux_logical_fallacies": {
      "original": 100,
      "effective": 100
    },
    "mmlu_redux_high_school_macroeconomics": {
      "original": 100,
      "effective": 100
    },
    "mmlu_redux_conceptual_physics": {
      "original": 100,
      "effective": 100
    },
    "mmlu_redux_human_aging": {
      "original": 100,
      "effective": 100
    },
    "mmlu_redux_philosophy": {
      "original": 100,
      "effective": 100
    },
    "mmlu_redux_virology": {
      "original": 100,
      "effective": 100
    },
    "mmlu_redux_clinical_knowledge": {
      "original": 100,
      "effective": 100
    },
    "mmlu_redux_college_chemistry": {
      "original": 100,
      "effective": 100
    },
    "mmlu_redux_astronomy": {
      "original": 100,
      "effective": 100
    },
    "mmlu_redux_public_relations": {
      "original": 100,
      "effective": 100
    },
    "mmlu_redux_miscellaneous": {
      "original": 100,
      "effective": 100
    },
    "mmlu_redux_college_medicine": {
      "original": 100,
      "effective": 100
    },
    "mmlu_redux_high_school_mathematics": {
      "original": 100,
      "effective": 100
    },
    "mmlu_redux_high_school_physics": {
      "original": 100,
      "effective": 100
    },
    "mmlu_redux_anatomy": {
      "original": 100,
      "effective": 100
    },
    "mmlu_redux_global_facts": {
      "original": 100,
      "effective": 100
    },
    "mmlu_redux_high_school_statistics": {
      "original": 100,
      "effective": 100
    },
    "mmlu_redux_high_school_us_history": {
      "original": 100,
      "effective": 100
    },
    "mmlu_redux_high_school_chemistry": {
      "original": 100,
      "effective": 100
    },
    "mmlu_redux_college_computer_science": {
      "original": 100,
      "effective": 100
    },
    "mmlu_redux_econometrics": {
      "original": 100,
      "effective": 100
    },
    "mmlu_redux_high_school_geography": {
      "original": 100,
      "effective": 100
    },
    "mmlu_redux_professional_accounting": {
      "original": 100,
      "effective": 100
    },
    "mmlu_redux_business_ethics": {
      "original": 100,
      "effective": 100
    },
    "mmlu_redux_formal_logic": {
      "original": 100,
      "effective": 100
    },
    "mmlu_redux_college_mathematics": {
      "original": 100,
      "effective": 100
    },
    "mmlu_redux_electrical_engineering": {
      "original": 100,
      "effective": 100
    },
    "mmlu_redux_college_physics": {
      "original": 100,
      "effective": 100
    }
  },
  "config": {
    "model": "hf",
    "model_args": "pretrained=mistralai/Mistral-7B-v0.3,parallelize=True,attn_implementation=sdpa",
    "model_num_parameters": 7248023552,
    "model_dtype": "torch.bfloat16",
    "model_revision": "main",
    "model_sha": "d8cadc02ac76bd617a919d50b092e59d2d110aff",
    "batch_size": 1,
    "batch_sizes": [],
    "device": "cuda",
    "use_cache": null,
    "limit": null,
    "bootstrap_iters": 100000,
    "gen_kwargs": null,
    "random_seed": 0,
    "numpy_seed": 1234,
    "torch_seed": 1234,
    "fewshot_seed": 1234
  },
  "git_hash": null,
  "date": 1733517573.42315,
  "pretty_env_info": "PyTorch version: 2.5.1+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Rocky Linux 8.5 (Green Obsidian) (x86_64)\nGCC version: (GCC) 8.5.0 20210514 (Red Hat 8.5.0-4)\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.28\n\nPython version: 3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-4.18.0-348.7.1.el8_5.x86_64-x86_64-with-glibc2.28\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: Tesla V100-PCIE-32GB\nGPU 1: Tesla V100-PCIE-32GB\nGPU 2: Tesla V100-PCIE-32GB\nGPU 3: Tesla V100-PCIE-32GB\n\nNvidia driver version: 535.104.12\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:        x86_64\nCPU op-mode(s):      32-bit, 64-bit\nByte Order:          Little Endian\nCPU(s):              48\nOn-line CPU(s) list: 0-47\nThread(s) per core:  2\nCore(s) per socket:  12\nSocket(s):           2\nNUMA node(s):        2\nVendor ID:           GenuineIntel\nCPU family:          6\nModel:               85\nModel name:          Intel(R) Xeon(R) Silver 4214 CPU @ 2.20GHz\nStepping:            7\nCPU MHz:             3200.000\nCPU max MHz:         3200.0000\nCPU min MHz:         1000.0000\nBogoMIPS:            4400.00\nVirtualization:      VT-x\nL1d cache:           32K\nL1i cache:           32K\nL2 cache:            1024K\nL3 cache:            16896K\nNUMA node0 CPU(s):   0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46\nNUMA node1 CPU(s):   1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47\nFlags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single intel_ppin ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid cqm mpx rdt_a avx512f avx512dq rdseed adx smap clflushopt clwb intel_pt avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts pku ospke avx512_vnni md_clear flush_l1d arch_capabilities\n\nVersions of relevant libraries:\n[pip3] numpy==2.1.3\n[pip3] torch==2.5.1\n[pip3] triton==3.1.0\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] optree                    0.11.0                   pypi_0    pypi\n[conda] torch                     2.4.1                    pypi_0    pypi\n[conda] torchvision               0.19.1                   pypi_0    pypi\n[conda] triton                    3.0.0                    pypi_0    pypi",
  "transformers_version": "4.46.2",
  "upper_git_hash": null,
  "tokenizer_pad_token": [
    "<unk>",
    "0"
  ],
  "tokenizer_eos_token": [
    "</s>",
    "2"
  ],
  "tokenizer_bos_token": [
    "<s>",
    "1"
  ],
  "eot_token_id": 2,
  "max_length": 32768,
  "task_hashes": {
    "mmlu_redux_machine_learning": "c8d45aed23b2c27d4d6fe23be7aff70e26170a1b8c2fedd72c97bc4e88091b70",
    "mmlu_redux_professional_law": "989d221b0eeb91fcba1988f0f5c918045d12b03320ef2a7bdf5171fcc3713a5f",
    "mmlu_redux_logical_fallacies": "a2388052e88a23a7195ac8fe2a06da3edcb2ad9ec422d9079a0221b28cee91c2",
    "mmlu_redux_high_school_macroeconomics": "91388b05ccef5fb792a033a3c74c3019a120f4994b538bd72817d9e84f72e8cc",
    "mmlu_redux_conceptual_physics": "b9faf120d4cda4dfb3d5fc5b9dca377ea1cab411ba5a729d97c66eaaed5465d9",
    "mmlu_redux_human_aging": "78f5f72ebc981c3cf65d177cbc37e4db937b92a984d75c0c2b08d6b07a2f25c1",
    "mmlu_redux_philosophy": "37d2210e68becb34ae70db33516bd9beb549b9a8b5a9eb0b45b9a1b4b5a29bdc",
    "mmlu_redux_virology": "3339fd165f26df3451343fd32593f1a7905ade4d7c95ee39e200374a55a04fa4",
    "mmlu_redux_clinical_knowledge": "7ad1cf129462d070ff61d5e3d20c5fe103329c273220d99011304ac928758c48",
    "mmlu_redux_college_chemistry": "141457ef621bdd9023d9fb5dbb3f37386ab4c28e9e652f9037bf5ed5db0aab09",
    "mmlu_redux_astronomy": "8ec44db51eb6bee5dd4abe648ed40fff77b41ea13dcd90ff51ac1c0f305d0a7d",
    "mmlu_redux_public_relations": "03667c3c0184260e0f443607edd7e65c57c41db19021bfc60c4415bf9fdc48cd",
    "mmlu_redux_miscellaneous": "95c1ac2cbc6822f628fdee2e94494a6e51843a8f381efa12b69bbd6cc76a9512",
    "mmlu_redux_college_medicine": "2741d11509c46498136a4570bbfe704775806749b1968234c7e0ec566e270e9e",
    "mmlu_redux_high_school_mathematics": "f96b067d6f7c38d4bf3c90c9e6207c35c472d37a92e64e4c4f5ecbe214d4c2d3",
    "mmlu_redux_high_school_physics": "bae8b97251dd42394eff7b6e53d018f214079c5ab401059d1674d387fc30f285",
    "mmlu_redux_anatomy": "00708b1d804aab0e4d9048cd4cb79e41c5035fab0c4288bbc11daf6eb7b7d24a",
    "mmlu_redux_global_facts": "f440bed2a2c851852b11687e3d5d05171f788398585492335dad1329468f7b5f",
    "mmlu_redux_high_school_statistics": "7315d1a3234ed78a6ae90f9fb132e40d1274de28a3528273ef39863c7e0b440a",
    "mmlu_redux_high_school_us_history": "65151a150b26f42b69e57820bf89dc4dc7af0786d3afdcea158481a9d5174966",
    "mmlu_redux_high_school_chemistry": "cdbf892567a2d007269ab240c996aa9a6bd30f78186b2ed3c6e174dc529ad3df",
    "mmlu_redux_college_computer_science": "f611ddf25e9e0a982971e56f3c0ae05856a86e803e1b0e738bfe22ef26ac54b6",
    "mmlu_redux_econometrics": "30ef7c56f4a93e66ad3e1a40f9d76123bac5188a69c8bf66b4f92c88c86e8e6a",
    "mmlu_redux_high_school_geography": "79b6813b6a673849868ca9a10d801e4367f7ffcdfe4d6aecb27a33bb524ef90e",
    "mmlu_redux_professional_accounting": "b6eeceaf3f2e06c5061a69388e1c5c479725d8382df53429533adba5e6c22f03",
    "mmlu_redux_business_ethics": "8451b407c16bca0b680d4542ebbb58fba5799d0aaedb733c17e22380407a459c",
    "mmlu_redux_formal_logic": "5692845303e409bf856f2450e0a1c2e1511317f5d7ad05eb9586b05d46d95588",
    "mmlu_redux_college_mathematics": "1aae588b77ba0dc3dabc5ea42411705b1a6472f6c58ba05b7024b9b522282900",
    "mmlu_redux_electrical_engineering": "7e46e97ad4d40880ff464ab4f8d12f1c33429781d5519be0f079ca6956e841b8",
    "mmlu_redux_college_physics": "3a40937f4e9e0f7e77e8c8b5239f5e485981861c955beaa951b9914435bde26c"
  },
  "model_source": "hf",
  "model_name": "mistralai/Mistral-7B-v0.3",
  "model_name_sanitized": "mistralai__Mistral-7B-v0.3",
  "system_instruction": null,
  "system_instruction_sha": null,
  "fewshot_as_multiturn": false,
  "chat_template": null,
  "chat_template_sha": null,
  "start_time": 9360721.963831795,
  "end_time": 9363623.592601504,
  "total_evaluation_time_seconds": "2901.6287697087973"
}