{
  "results": {
    "mmlu_redux": {
      "alias": "mmlu_redux"
    },
    "mmlu_redux_anatomy": {
      "alias": " - anatomy",
      "acc,none": 0.49,
      "acc_stderr,none": 0.05024183937956912
    },
    "mmlu_redux_astronomy": {
      "alias": " - astronomy",
      "acc,none": 0.55,
      "acc_stderr,none": 0.05
    },
    "mmlu_redux_business_ethics": {
      "alias": " - business_ethics",
      "acc,none": 0.62,
      "acc_stderr,none": 0.04878317312145632
    },
    "mmlu_redux_clinical_knowledge": {
      "alias": " - clinical_knowledge",
      "acc,none": 0.57,
      "acc_stderr,none": 0.04975698519562428
    },
    "mmlu_redux_college_chemistry": {
      "alias": " - college_chemistry",
      "acc,none": 0.29,
      "acc_stderr,none": 0.04560480215720684
    },
    "mmlu_redux_college_computer_science": {
      "alias": " - college_computer_science",
      "acc,none": 0.51,
      "acc_stderr,none": 0.05024183937956912
    },
    "mmlu_redux_college_mathematics": {
      "alias": " - college_mathematics",
      "acc,none": 0.38,
      "acc_stderr,none": 0.048783173121456316
    },
    "mmlu_redux_college_medicine": {
      "alias": " - college_medicine",
      "acc,none": 0.63,
      "acc_stderr,none": 0.048523658709391
    },
    "mmlu_redux_college_physics": {
      "alias": " - college_physics",
      "acc,none": 0.33,
      "acc_stderr,none": 0.047258156262526045
    },
    "mmlu_redux_conceptual_physics": {
      "alias": " - conceptual_physics",
      "acc,none": 0.45,
      "acc_stderr,none": 0.05
    },
    "mmlu_redux_econometrics": {
      "alias": " - econometrics",
      "acc,none": 0.39,
      "acc_stderr,none": 0.04902071300001975
    },
    "mmlu_redux_electrical_engineering": {
      "alias": " - electrical_engineering",
      "acc,none": 0.5,
      "acc_stderr,none": 0.050251890762960605
    },
    "mmlu_redux_formal_logic": {
      "alias": " - formal_logic",
      "acc,none": 0.51,
      "acc_stderr,none": 0.05024183937956912
    },
    "mmlu_redux_global_facts": {
      "alias": " - global_facts",
      "acc,none": 0.35,
      "acc_stderr,none": 0.047937248544110175
    },
    "mmlu_redux_high_school_chemistry": {
      "alias": " - high_school_chemistry",
      "acc,none": 0.44,
      "acc_stderr,none": 0.04988876515698589
    },
    "mmlu_redux_high_school_geography": {
      "alias": " - high_school_geography",
      "acc,none": 0.75,
      "acc_stderr,none": 0.04351941398892446
    },
    "mmlu_redux_high_school_macroeconomics": {
      "alias": " - high_school_macroeconomics",
      "acc,none": 0.48,
      "acc_stderr,none": 0.050211673156867795
    },
    "mmlu_redux_high_school_mathematics": {
      "alias": " - high_school_mathematics",
      "acc,none": 0.35,
      "acc_stderr,none": 0.047937248544110196
    },
    "mmlu_redux_high_school_physics": {
      "alias": " - high_school_physics",
      "acc,none": 0.37,
      "acc_stderr,none": 0.04852365870939098
    },
    "mmlu_redux_high_school_statistics": {
      "alias": " - high_school_statistics",
      "acc,none": 0.47,
      "acc_stderr,none": 0.050161355804659205
    },
    "mmlu_redux_high_school_us_history": {
      "alias": " - high_school_us_history",
      "acc,none": 0.73,
      "acc_stderr,none": 0.0446196043338474
    },
    "mmlu_redux_human_aging": {
      "alias": " - human_aging",
      "acc,none": 0.63,
      "acc_stderr,none": 0.04852365870939098
    },
    "mmlu_redux_logical_fallacies": {
      "alias": " - logical_fallacies",
      "acc,none": 0.72,
      "acc_stderr,none": 0.04512608598542127
    },
    "mmlu_redux_machine_learning": {
      "alias": " - machine_learning",
      "acc,none": 0.46,
      "acc_stderr,none": 0.05009082659620332
    },
    "mmlu_redux_miscellaneous": {
      "alias": " - miscellaneous",
      "acc,none": 0.76,
      "acc_stderr,none": 0.04292346959909284
    },
    "mmlu_redux_philosophy": {
      "alias": " - philosophy",
      "acc,none": 0.59,
      "acc_stderr,none": 0.04943110704237101
    },
    "mmlu_redux_professional_accounting": {
      "alias": " - professional_accounting",
      "acc,none": 0.41,
      "acc_stderr,none": 0.049431107042371025
    },
    "mmlu_redux_professional_law": {
      "alias": " - professional_law",
      "acc,none": 0.4,
      "acc_stderr,none": 0.04923659639173309
    },
    "mmlu_redux_public_relations": {
      "alias": " - public_relations",
      "acc,none": 0.69,
      "acc_stderr,none": 0.04648231987117316
    },
    "mmlu_redux_virology": {
      "alias": " - virology",
      "acc,none": 0.42,
      "acc_stderr,none": 0.04960449637488585
    }
  },
  "groups": {
    "mmlu_redux": {
      "alias": "mmlu_redux"
    }
  },
  "group_subtasks": {
    "mmlu_redux": [
      "mmlu_redux_machine_learning",
      "mmlu_redux_professional_law",
      "mmlu_redux_logical_fallacies",
      "mmlu_redux_high_school_macroeconomics",
      "mmlu_redux_conceptual_physics",
      "mmlu_redux_human_aging",
      "mmlu_redux_philosophy",
      "mmlu_redux_virology",
      "mmlu_redux_clinical_knowledge",
      "mmlu_redux_college_chemistry",
      "mmlu_redux_astronomy",
      "mmlu_redux_public_relations",
      "mmlu_redux_miscellaneous",
      "mmlu_redux_college_medicine",
      "mmlu_redux_high_school_mathematics",
      "mmlu_redux_high_school_physics",
      "mmlu_redux_anatomy",
      "mmlu_redux_global_facts",
      "mmlu_redux_high_school_statistics",
      "mmlu_redux_high_school_us_history",
      "mmlu_redux_high_school_chemistry",
      "mmlu_redux_college_computer_science",
      "mmlu_redux_econometrics",
      "mmlu_redux_high_school_geography",
      "mmlu_redux_professional_accounting",
      "mmlu_redux_business_ethics",
      "mmlu_redux_formal_logic",
      "mmlu_redux_college_mathematics",
      "mmlu_redux_electrical_engineering",
      "mmlu_redux_college_physics"
    ]
  },
  "configs": {
    "mmlu_redux_anatomy": {
      "task": "mmlu_redux_anatomy",
      "task_alias": "anatomy",
      "dataset_path": "edinburgh-dawg/mmlu-redux",
      "dataset_name": "anatomy",
      "dataset_kwargs": {
        "trust_remote_code": true
      },
      "test_split": "test",
      "fewshot_split": "test",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0
      }
    },
    "mmlu_redux_astronomy": {
      "task": "mmlu_redux_astronomy",
      "task_alias": "astronomy",
      "dataset_path": "edinburgh-dawg/mmlu-redux",
      "dataset_name": "astronomy",
      "dataset_kwargs": {
        "trust_remote_code": true
      },
      "test_split": "test",
      "fewshot_split": "test",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0
      }
    },
    "mmlu_redux_business_ethics": {
      "task": "mmlu_redux_business_ethics",
      "task_alias": "business_ethics",
      "dataset_path": "edinburgh-dawg/mmlu-redux",
      "dataset_name": "business_ethics",
      "dataset_kwargs": {
        "trust_remote_code": true
      },
      "test_split": "test",
      "fewshot_split": "test",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0
      }
    },
    "mmlu_redux_clinical_knowledge": {
      "task": "mmlu_redux_clinical_knowledge",
      "task_alias": "clinical_knowledge",
      "dataset_path": "edinburgh-dawg/mmlu-redux",
      "dataset_name": "clinical_knowledge",
      "dataset_kwargs": {
        "trust_remote_code": true
      },
      "test_split": "test",
      "fewshot_split": "test",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0
      }
    },
    "mmlu_redux_college_chemistry": {
      "task": "mmlu_redux_college_chemistry",
      "task_alias": "college_chemistry",
      "dataset_path": "edinburgh-dawg/mmlu-redux",
      "dataset_name": "college_chemistry",
      "dataset_kwargs": {
        "trust_remote_code": true
      },
      "test_split": "test",
      "fewshot_split": "test",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0
      }
    },
    "mmlu_redux_college_computer_science": {
      "task": "mmlu_redux_college_computer_science",
      "task_alias": "college_computer_science",
      "dataset_path": "edinburgh-dawg/mmlu-redux",
      "dataset_name": "college_computer_science",
      "dataset_kwargs": {
        "trust_remote_code": true
      },
      "test_split": "test",
      "fewshot_split": "test",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0
      }
    },
    "mmlu_redux_college_mathematics": {
      "task": "mmlu_redux_college_mathematics",
      "task_alias": "college_mathematics",
      "dataset_path": "edinburgh-dawg/mmlu-redux",
      "dataset_name": "college_mathematics",
      "dataset_kwargs": {
        "trust_remote_code": true
      },
      "test_split": "test",
      "fewshot_split": "test",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0
      }
    },
    "mmlu_redux_college_medicine": {
      "task": "mmlu_redux_college_medicine",
      "task_alias": "college_medicine",
      "dataset_path": "edinburgh-dawg/mmlu-redux",
      "dataset_name": "college_medicine",
      "dataset_kwargs": {
        "trust_remote_code": true
      },
      "test_split": "test",
      "fewshot_split": "test",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0
      }
    },
    "mmlu_redux_college_physics": {
      "task": "mmlu_redux_college_physics",
      "task_alias": "college_physics",
      "dataset_path": "edinburgh-dawg/mmlu-redux",
      "dataset_name": "college_physics",
      "dataset_kwargs": {
        "trust_remote_code": true
      },
      "test_split": "test",
      "fewshot_split": "test",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0
      }
    },
    "mmlu_redux_conceptual_physics": {
      "task": "mmlu_redux_conceptual_physics",
      "task_alias": "conceptual_physics",
      "dataset_path": "edinburgh-dawg/mmlu-redux",
      "dataset_name": "conceptual_physics",
      "dataset_kwargs": {
        "trust_remote_code": true
      },
      "test_split": "test",
      "fewshot_split": "test",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0
      }
    },
    "mmlu_redux_econometrics": {
      "task": "mmlu_redux_econometrics",
      "task_alias": "econometrics",
      "dataset_path": "edinburgh-dawg/mmlu-redux",
      "dataset_name": "econometrics",
      "dataset_kwargs": {
        "trust_remote_code": true
      },
      "test_split": "test",
      "fewshot_split": "test",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0
      }
    },
    "mmlu_redux_electrical_engineering": {
      "task": "mmlu_redux_electrical_engineering",
      "task_alias": "electrical_engineering",
      "dataset_path": "edinburgh-dawg/mmlu-redux",
      "dataset_name": "electrical_engineering",
      "dataset_kwargs": {
        "trust_remote_code": true
      },
      "test_split": "test",
      "fewshot_split": "test",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0
      }
    },
    "mmlu_redux_formal_logic": {
      "task": "mmlu_redux_formal_logic",
      "task_alias": "formal_logic",
      "dataset_path": "edinburgh-dawg/mmlu-redux",
      "dataset_name": "formal_logic",
      "dataset_kwargs": {
        "trust_remote_code": true
      },
      "test_split": "test",
      "fewshot_split": "test",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0
      }
    },
    "mmlu_redux_global_facts": {
      "task": "mmlu_redux_global_facts",
      "task_alias": "global_facts",
      "dataset_path": "edinburgh-dawg/mmlu-redux",
      "dataset_name": "global_facts",
      "dataset_kwargs": {
        "trust_remote_code": true
      },
      "test_split": "test",
      "fewshot_split": "test",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0
      }
    },
    "mmlu_redux_high_school_chemistry": {
      "task": "mmlu_redux_high_school_chemistry",
      "task_alias": "high_school_chemistry",
      "dataset_path": "edinburgh-dawg/mmlu-redux",
      "dataset_name": "high_school_chemistry",
      "dataset_kwargs": {
        "trust_remote_code": true
      },
      "test_split": "test",
      "fewshot_split": "test",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0
      }
    },
    "mmlu_redux_high_school_geography": {
      "task": "mmlu_redux_high_school_geography",
      "task_alias": "high_school_geography",
      "dataset_path": "edinburgh-dawg/mmlu-redux",
      "dataset_name": "high_school_geography",
      "dataset_kwargs": {
        "trust_remote_code": true
      },
      "test_split": "test",
      "fewshot_split": "test",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0
      }
    },
    "mmlu_redux_high_school_macroeconomics": {
      "task": "mmlu_redux_high_school_macroeconomics",
      "task_alias": "high_school_macroeconomics",
      "dataset_path": "edinburgh-dawg/mmlu-redux",
      "dataset_name": "high_school_macroeconomics",
      "dataset_kwargs": {
        "trust_remote_code": true
      },
      "test_split": "test",
      "fewshot_split": "test",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0
      }
    },
    "mmlu_redux_high_school_mathematics": {
      "task": "mmlu_redux_high_school_mathematics",
      "task_alias": "high_school_mathematics",
      "dataset_path": "edinburgh-dawg/mmlu-redux",
      "dataset_name": "high_school_mathematics",
      "dataset_kwargs": {
        "trust_remote_code": true
      },
      "test_split": "test",
      "fewshot_split": "test",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0
      }
    },
    "mmlu_redux_high_school_physics": {
      "task": "mmlu_redux_high_school_physics",
      "task_alias": "high_school_physics",
      "dataset_path": "edinburgh-dawg/mmlu-redux",
      "dataset_name": "high_school_physics",
      "dataset_kwargs": {
        "trust_remote_code": true
      },
      "test_split": "test",
      "fewshot_split": "test",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0
      }
    },
    "mmlu_redux_high_school_statistics": {
      "task": "mmlu_redux_high_school_statistics",
      "task_alias": "high_school_statistics",
      "dataset_path": "edinburgh-dawg/mmlu-redux",
      "dataset_name": "high_school_statistics",
      "dataset_kwargs": {
        "trust_remote_code": true
      },
      "test_split": "test",
      "fewshot_split": "test",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0
      }
    },
    "mmlu_redux_high_school_us_history": {
      "task": "mmlu_redux_high_school_us_history",
      "task_alias": "high_school_us_history",
      "dataset_path": "edinburgh-dawg/mmlu-redux",
      "dataset_name": "high_school_us_history",
      "dataset_kwargs": {
        "trust_remote_code": true
      },
      "test_split": "test",
      "fewshot_split": "test",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0
      }
    },
    "mmlu_redux_human_aging": {
      "task": "mmlu_redux_human_aging",
      "task_alias": "human_aging",
      "dataset_path": "edinburgh-dawg/mmlu-redux",
      "dataset_name": "human_aging",
      "dataset_kwargs": {
        "trust_remote_code": true
      },
      "test_split": "test",
      "fewshot_split": "test",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0
      }
    },
    "mmlu_redux_logical_fallacies": {
      "task": "mmlu_redux_logical_fallacies",
      "task_alias": "logical_fallacies",
      "dataset_path": "edinburgh-dawg/mmlu-redux",
      "dataset_name": "logical_fallacies",
      "dataset_kwargs": {
        "trust_remote_code": true
      },
      "test_split": "test",
      "fewshot_split": "test",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0
      }
    },
    "mmlu_redux_machine_learning": {
      "task": "mmlu_redux_machine_learning",
      "task_alias": "machine_learning",
      "dataset_path": "edinburgh-dawg/mmlu-redux",
      "dataset_name": "machine_learning",
      "dataset_kwargs": {
        "trust_remote_code": true
      },
      "test_split": "test",
      "fewshot_split": "test",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0
      }
    },
    "mmlu_redux_miscellaneous": {
      "task": "mmlu_redux_miscellaneous",
      "task_alias": "miscellaneous",
      "dataset_path": "edinburgh-dawg/mmlu-redux",
      "dataset_name": "miscellaneous",
      "dataset_kwargs": {
        "trust_remote_code": true
      },
      "test_split": "test",
      "fewshot_split": "test",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0
      }
    },
    "mmlu_redux_philosophy": {
      "task": "mmlu_redux_philosophy",
      "task_alias": "philosophy",
      "dataset_path": "edinburgh-dawg/mmlu-redux",
      "dataset_name": "philosophy",
      "dataset_kwargs": {
        "trust_remote_code": true
      },
      "test_split": "test",
      "fewshot_split": "test",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0
      }
    },
    "mmlu_redux_professional_accounting": {
      "task": "mmlu_redux_professional_accounting",
      "task_alias": "professional_accounting",
      "dataset_path": "edinburgh-dawg/mmlu-redux",
      "dataset_name": "professional_accounting",
      "dataset_kwargs": {
        "trust_remote_code": true
      },
      "test_split": "test",
      "fewshot_split": "test",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0
      }
    },
    "mmlu_redux_professional_law": {
      "task": "mmlu_redux_professional_law",
      "task_alias": "professional_law",
      "dataset_path": "edinburgh-dawg/mmlu-redux",
      "dataset_name": "professional_law",
      "dataset_kwargs": {
        "trust_remote_code": true
      },
      "test_split": "test",
      "fewshot_split": "test",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0
      }
    },
    "mmlu_redux_public_relations": {
      "task": "mmlu_redux_public_relations",
      "task_alias": "public_relations",
      "dataset_path": "edinburgh-dawg/mmlu-redux",
      "dataset_name": "public_relations",
      "dataset_kwargs": {
        "trust_remote_code": true
      },
      "test_split": "test",
      "fewshot_split": "test",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0
      }
    },
    "mmlu_redux_virology": {
      "task": "mmlu_redux_virology",
      "task_alias": "virology",
      "dataset_path": "edinburgh-dawg/mmlu-redux",
      "dataset_name": "virology",
      "dataset_kwargs": {
        "trust_remote_code": true
      },
      "test_split": "test",
      "fewshot_split": "test",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0
      }
    }
  },
  "versions": {
    "mmlu_redux": 2.0,
    "mmlu_redux_anatomy": 1.0,
    "mmlu_redux_astronomy": 1.0,
    "mmlu_redux_business_ethics": 1.0,
    "mmlu_redux_clinical_knowledge": 1.0,
    "mmlu_redux_college_chemistry": 1.0,
    "mmlu_redux_college_computer_science": 1.0,
    "mmlu_redux_college_mathematics": 1.0,
    "mmlu_redux_college_medicine": 1.0,
    "mmlu_redux_college_physics": 1.0,
    "mmlu_redux_conceptual_physics": 1.0,
    "mmlu_redux_econometrics": 1.0,
    "mmlu_redux_electrical_engineering": 1.0,
    "mmlu_redux_formal_logic": 1.0,
    "mmlu_redux_global_facts": 1.0,
    "mmlu_redux_high_school_chemistry": 1.0,
    "mmlu_redux_high_school_geography": 1.0,
    "mmlu_redux_high_school_macroeconomics": 1.0,
    "mmlu_redux_high_school_mathematics": 1.0,
    "mmlu_redux_high_school_physics": 1.0,
    "mmlu_redux_high_school_statistics": 1.0,
    "mmlu_redux_high_school_us_history": 1.0,
    "mmlu_redux_human_aging": 1.0,
    "mmlu_redux_logical_fallacies": 1.0,
    "mmlu_redux_machine_learning": 1.0,
    "mmlu_redux_miscellaneous": 1.0,
    "mmlu_redux_philosophy": 1.0,
    "mmlu_redux_professional_accounting": 1.0,
    "mmlu_redux_professional_law": 1.0,
    "mmlu_redux_public_relations": 1.0,
    "mmlu_redux_virology": 1.0
  },
  "n-shot": {
    "mmlu_redux_anatomy": 5,
    "mmlu_redux_astronomy": 5,
    "mmlu_redux_business_ethics": 5,
    "mmlu_redux_clinical_knowledge": 5,
    "mmlu_redux_college_chemistry": 5,
    "mmlu_redux_college_computer_science": 5,
    "mmlu_redux_college_mathematics": 5,
    "mmlu_redux_college_medicine": 5,
    "mmlu_redux_college_physics": 5,
    "mmlu_redux_conceptual_physics": 5,
    "mmlu_redux_econometrics": 5,
    "mmlu_redux_electrical_engineering": 5,
    "mmlu_redux_formal_logic": 5,
    "mmlu_redux_global_facts": 5,
    "mmlu_redux_high_school_chemistry": 5,
    "mmlu_redux_high_school_geography": 5,
    "mmlu_redux_high_school_macroeconomics": 5,
    "mmlu_redux_high_school_mathematics": 5,
    "mmlu_redux_high_school_physics": 5,
    "mmlu_redux_high_school_statistics": 5,
    "mmlu_redux_high_school_us_history": 5,
    "mmlu_redux_human_aging": 5,
    "mmlu_redux_logical_fallacies": 5,
    "mmlu_redux_machine_learning": 5,
    "mmlu_redux_miscellaneous": 5,
    "mmlu_redux_philosophy": 5,
    "mmlu_redux_professional_accounting": 5,
    "mmlu_redux_professional_law": 5,
    "mmlu_redux_public_relations": 5,
    "mmlu_redux_virology": 5
  },
  "higher_is_better": {
    "mmlu_redux": {
      "acc": true
    },
    "mmlu_redux_anatomy": {
      "acc": true
    },
    "mmlu_redux_astronomy": {
      "acc": true
    },
    "mmlu_redux_business_ethics": {
      "acc": true
    },
    "mmlu_redux_clinical_knowledge": {
      "acc": true
    },
    "mmlu_redux_college_chemistry": {
      "acc": true
    },
    "mmlu_redux_college_computer_science": {
      "acc": true
    },
    "mmlu_redux_college_mathematics": {
      "acc": true
    },
    "mmlu_redux_college_medicine": {
      "acc": true
    },
    "mmlu_redux_college_physics": {
      "acc": true
    },
    "mmlu_redux_conceptual_physics": {
      "acc": true
    },
    "mmlu_redux_econometrics": {
      "acc": true
    },
    "mmlu_redux_electrical_engineering": {
      "acc": true
    },
    "mmlu_redux_formal_logic": {
      "acc": true
    },
    "mmlu_redux_global_facts": {
      "acc": true
    },
    "mmlu_redux_high_school_chemistry": {
      "acc": true
    },
    "mmlu_redux_high_school_geography": {
      "acc": true
    },
    "mmlu_redux_high_school_macroeconomics": {
      "acc": true
    },
    "mmlu_redux_high_school_mathematics": {
      "acc": true
    },
    "mmlu_redux_high_school_physics": {
      "acc": true
    },
    "mmlu_redux_high_school_statistics": {
      "acc": true
    },
    "mmlu_redux_high_school_us_history": {
      "acc": true
    },
    "mmlu_redux_human_aging": {
      "acc": true
    },
    "mmlu_redux_logical_fallacies": {
      "acc": true
    },
    "mmlu_redux_machine_learning": {
      "acc": true
    },
    "mmlu_redux_miscellaneous": {
      "acc": true
    },
    "mmlu_redux_philosophy": {
      "acc": true
    },
    "mmlu_redux_professional_accounting": {
      "acc": true
    },
    "mmlu_redux_professional_law": {
      "acc": true
    },
    "mmlu_redux_public_relations": {
      "acc": true
    },
    "mmlu_redux_virology": {
      "acc": true
    }
  },
  "n-samples": {
    "mmlu_redux_machine_learning": {
      "original": 100,
      "effective": 100
    },
    "mmlu_redux_professional_law": {
      "original": 100,
      "effective": 100
    },
    "mmlu_redux_logical_fallacies": {
      "original": 100,
      "effective": 100
    },
    "mmlu_redux_high_school_macroeconomics": {
      "original": 100,
      "effective": 100
    },
    "mmlu_redux_conceptual_physics": {
      "original": 100,
      "effective": 100
    },
    "mmlu_redux_human_aging": {
      "original": 100,
      "effective": 100
    },
    "mmlu_redux_philosophy": {
      "original": 100,
      "effective": 100
    },
    "mmlu_redux_virology": {
      "original": 100,
      "effective": 100
    },
    "mmlu_redux_clinical_knowledge": {
      "original": 100,
      "effective": 100
    },
    "mmlu_redux_college_chemistry": {
      "original": 100,
      "effective": 100
    },
    "mmlu_redux_astronomy": {
      "original": 100,
      "effective": 100
    },
    "mmlu_redux_public_relations": {
      "original": 100,
      "effective": 100
    },
    "mmlu_redux_miscellaneous": {
      "original": 100,
      "effective": 100
    },
    "mmlu_redux_college_medicine": {
      "original": 100,
      "effective": 100
    },
    "mmlu_redux_high_school_mathematics": {
      "original": 100,
      "effective": 100
    },
    "mmlu_redux_high_school_physics": {
      "original": 100,
      "effective": 100
    },
    "mmlu_redux_anatomy": {
      "original": 100,
      "effective": 100
    },
    "mmlu_redux_global_facts": {
      "original": 100,
      "effective": 100
    },
    "mmlu_redux_high_school_statistics": {
      "original": 100,
      "effective": 100
    },
    "mmlu_redux_high_school_us_history": {
      "original": 100,
      "effective": 100
    },
    "mmlu_redux_high_school_chemistry": {
      "original": 100,
      "effective": 100
    },
    "mmlu_redux_college_computer_science": {
      "original": 100,
      "effective": 100
    },
    "mmlu_redux_econometrics": {
      "original": 100,
      "effective": 100
    },
    "mmlu_redux_high_school_geography": {
      "original": 100,
      "effective": 100
    },
    "mmlu_redux_professional_accounting": {
      "original": 100,
      "effective": 100
    },
    "mmlu_redux_business_ethics": {
      "original": 100,
      "effective": 100
    },
    "mmlu_redux_formal_logic": {
      "original": 100,
      "effective": 100
    },
    "mmlu_redux_college_mathematics": {
      "original": 100,
      "effective": 100
    },
    "mmlu_redux_electrical_engineering": {
      "original": 100,
      "effective": 100
    },
    "mmlu_redux_college_physics": {
      "original": 100,
      "effective": 100
    }
  },
  "config": {
    "model": "hf",
    "model_args": "pretrained=deepseek-ai/DeepSeek-R1-Distill-Llama-8B,parallelize=True,attn_implementation=sdpa",
    "model_num_parameters": 8030261248,
    "model_dtype": "torch.bfloat16",
    "model_revision": "main",
    "model_sha": "24ae87a9c340aa4207dd46509414c019998e0161",
    "batch_size": 1,
    "batch_sizes": [],
    "device": "cuda",
    "use_cache": null,
    "limit": null,
    "bootstrap_iters": 100000,
    "gen_kwargs": null,
    "random_seed": 0,
    "numpy_seed": 1234,
    "torch_seed": 1234,
    "fewshot_seed": 1234
  },
  "git_hash": "f0bcb72",
  "date": 1737898754.2122686,
  "pretty_env_info": "PyTorch version: 2.5.1+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Rocky Linux 8.4 (Green Obsidian) (x86_64)\nGCC version: (GCC) 8.5.0 20210514 (Red Hat 8.5.0-4)\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.28\n\nPython version: 3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-4.18.0-305.19.1.el8_4.x86_64-x86_64-with-glibc2.28\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA A100-SXM4-80GB\nGPU 1: NVIDIA A100-SXM4-80GB\nGPU 2: NVIDIA A100-SXM4-80GB\nGPU 3: NVIDIA A100-SXM4-80GB\n\nNvidia driver version: 535.104.12\ncuDNN version: Probably one of the following:\n/usr/local/cuda-11.3/targets/x86_64-linux/lib/libcudnn.so.8.2.1\n/usr/local/cuda-11.3/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8.2.1\n/usr/local/cuda-11.3/targets/x86_64-linux/lib/libcudnn_adv_train.so.8.2.1\n/usr/local/cuda-11.3/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8.2.1\n/usr/local/cuda-11.3/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8.2.1\n/usr/local/cuda-11.3/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8.2.1\n/usr/local/cuda-11.3/targets/x86_64-linux/lib/libcudnn_ops_train.so.8.2.1\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:        x86_64\nCPU op-mode(s):      32-bit, 64-bit\nByte Order:          Little Endian\nCPU(s):              128\nOn-line CPU(s) list: 0-127\nThread(s) per core:  2\nCore(s) per socket:  32\nSocket(s):           2\nNUMA node(s):        2\nVendor ID:           AuthenticAMD\nCPU family:          25\nModel:               1\nModel name:          AMD EPYC 7513 32-Core Processor\nStepping:            1\nCPU MHz:             3475.707\nCPU max MHz:         2600.0000\nCPU min MHz:         1500.0000\nBogoMIPS:            5190.38\nVirtualization:      AMD-V\nL1d cache:           32K\nL1i cache:           32K\nL2 cache:            512K\nL3 cache:            32768K\nNUMA node0 CPU(s):   0-31,64-95\nNUMA node1 CPU(s):   32-63,96-127\nFlags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 invpcid_single hw_pstate sme ssbd mba sev ibrs ibpb stibp vmmcall sev_es fsgsbase bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold v_vmsave_vmload vgif umip pku ospke vaes vpclmulqdq rdpid overflow_recov succor smca fsrm\n\nVersions of relevant libraries:\n[pip3] numpy==2.1.3\n[pip3] torch==2.5.1\n[pip3] triton==3.1.0\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] optree                    0.11.0                   pypi_0    pypi\n[conda] torch                     2.4.1                    pypi_0    pypi\n[conda] torchvision               0.19.1                   pypi_0    pypi\n[conda] triton                    3.0.0                    pypi_0    pypi",
  "transformers_version": "4.46.2",
  "upper_git_hash": null,
  "tokenizer_pad_token": [
    "<｜end▁of▁sentence｜>",
    "128001"
  ],
  "tokenizer_eos_token": [
    "<｜end▁of▁sentence｜>",
    "128001"
  ],
  "tokenizer_bos_token": [
    "<｜begin▁of▁sentence｜>",
    "128000"
  ],
  "eot_token_id": 128001,
  "max_length": 131072,
  "task_hashes": {
    "mmlu_redux_machine_learning": "4969a3843ee7db09f2aeb0db32c0dd77eaafe6255e11c24ef50ed0e1ad139605",
    "mmlu_redux_professional_law": "35f14c26374b23e3b43e3541f8840b1d1bd33f7305df24258efb4b64438b3fbd",
    "mmlu_redux_logical_fallacies": "2cd00658224acca20ca465d19e268794870ede749152c480dd66ebd63121f7fb",
    "mmlu_redux_high_school_macroeconomics": "6ce276c7ee3f5aec70499764ad78663533534951bc076146232047b595e079c2",
    "mmlu_redux_conceptual_physics": "388b4e56be14b487892ef7a198647cb4d0a5d843de3358b4db3816ba8dd5be8b",
    "mmlu_redux_human_aging": "b1b233d591f6aafa6ea99d8cd93816a70f27029f79b62fe5882976e8e4f51937",
    "mmlu_redux_philosophy": "60ede03b31119ca09bc443baa0cdcae835cd400198a0799b06b6f4196cea50d7",
    "mmlu_redux_virology": "487ff79b9bd26e457f8abb30b0fa019e57495d720c9902bd2e18ad47936cef58",
    "mmlu_redux_clinical_knowledge": "2964189408253643eb3b8617c83e3bf5921cee1ed634d4358cdf37f1f80262f0",
    "mmlu_redux_college_chemistry": "1c7e1e830cadc08ffa943e73868bb51304f73da6a9e7d746f6c7bced9059c73c",
    "mmlu_redux_astronomy": "9f61f07197eb83c78fab6b528b48708ce1ab494f2a0ae4d39c10dc6f2a738fab",
    "mmlu_redux_public_relations": "8b56afd10e1a58b96bba995ce1ff35fc1f64ecc7bae6d7e8ce1b4e0252f55229",
    "mmlu_redux_miscellaneous": "84639d021b1d572209509a1ff689e29aab0fbd7dae629370c1dac3638a3a6613",
    "mmlu_redux_college_medicine": "bdd2594d5a7f1ef712730e4704bb345334e5afdf431d4d2b64d8a564939d3e1f",
    "mmlu_redux_high_school_mathematics": "22244f96edf883319d8ac8f5dbdd22174a0b437b87b61791f031d613efa6d3ac",
    "mmlu_redux_high_school_physics": "02744d35711aeeb57e0f68c6203a165eb229b233b2adf3653f39caed305b2c70",
    "mmlu_redux_anatomy": "bdbba96f289bf0b11c8697b4f49415614357a3c1bd3b5e2b36c871578cc950f6",
    "mmlu_redux_global_facts": "53fa23e1eb09f031be78560c390b60273f1f3b3575757ed9f5fb104400d91464",
    "mmlu_redux_high_school_statistics": "c4554a6fdbe20c635f3ea48926f85236052d190bb6e21eaec019e164a403faad",
    "mmlu_redux_high_school_us_history": "f401e56b021b5f3772367a7f4488c0e16de7cbeb943242b94dcb8165b06eaf57",
    "mmlu_redux_high_school_chemistry": "8f9548652789d505ee205f8a8ec9196c7933860a2afd371355d7fbfe50e21dd8",
    "mmlu_redux_college_computer_science": "8542b6272a357415059a383afc130dead2ecfd1f014cf6a3111975adab24d8f2",
    "mmlu_redux_econometrics": "858dcd268e465d9ebf1ea2636092c961b096caa753aacae42cf66e9a5da6a476",
    "mmlu_redux_high_school_geography": "2a7f0395a21306ac23f1e70ae667a61d38f3d65dfb4842bfdec39473a7487757",
    "mmlu_redux_professional_accounting": "43653492e0821a222306c9d353d34832756ac43ceb333292be5710f5913a8e8d",
    "mmlu_redux_business_ethics": "169a35d14ae4d49522b888fe3746e99009ba4e823a4c097e24432d15006503eb",
    "mmlu_redux_formal_logic": "1c93017e580530d16ec7c0fc2ca75c48dbdefd2aa353e4d92642e3a800f6b72e",
    "mmlu_redux_college_mathematics": "3e7e49b2a3f375a8db44ad0d06c8d086772f63f0f7b811bcf2ebb10b64fcf52b",
    "mmlu_redux_electrical_engineering": "53cd1c023f24b4aabd119f5555a97c5c6ac764acb1477b89615d9035eb79c82f",
    "mmlu_redux_college_physics": "70ba3a34db271f834037ac9a17cc558baf97c6de7f56091d0f5fe867eb5cef59"
  },
  "model_source": "hf",
  "model_name": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B",
  "model_name_sanitized": "deepseek-ai__DeepSeek-R1-Distill-Llama-8B",
  "system_instruction": null,
  "system_instruction_sha": null,
  "fewshot_as_multiturn": false,
  "chat_template": null,
  "chat_template_sha": null,
  "start_time": 11135823.640617372,
  "end_time": 11136088.853272704,
  "total_evaluation_time_seconds": "265.21265533193946"
}